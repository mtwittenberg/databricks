
#imports
from pyspark.sql.window import Window
from pyspark.sql.functions import approx_count_distinct, concat, col, lit, regexp_replace
from threading import Thread



#empty directory for new models
%fs rm -r "/FileStore/tables/models/"



#read in read data from blob storage
df = spark.read.parquet("/mnt/forecast/ts/timeseries_features").cache()
df.printSchema()



#Sorting the dataframe
df = df.orderBy("PairIdx","time").cache()



#create the pipeline  
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoderEstimator
from pyspark.ml.regression import GBTRegressor, GBTRegressionModel
from pyspark.ml.evaluation import RegressionEvaluator

categoricalColumns = ["Month","PairIdx"]

stages = [] # stages in our Pipeline
for categoricalCol in categoricalColumns:
    # Category Indexing with StringIndexer
    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + "Index").setHandleInvalid('keep')
    # Use OneHotEncoder to convert categorical variables into binary Sparse Vectors
    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + "classVec"])
    # Add stages, run all at once later on.
    stages += [stringIndexer, encoder]

#numericCols = ["SalesQty_lag1","SalesQty_median"]
numericCols = ["qtyInterp_local_lag1","qtyInterp_local_median"]

assemblerInputs = [c + "classVec" for c in categoricalColumns] + numericCols 
assembler = VectorAssembler(inputCols=assemblerInputs, outputCol="features")

stages += [assembler]

gbt = GBTRegressor(labelCol="label",featuresCol="features",maxIter=5,maxDepth=12,seed=42)

gbt_pipeline = Pipeline(stages=stages + [gbt])

#Metrics
# evaluatorRmse = RegressionEvaluator(predictionCol='prediction', labelCol='label',metricName='rmse')
# evaluatorR2 = RegressionEvaluator(predictionCol='prediction', labelCol='label',metricName='r2')
# evaluatorMse = RegressionEvaluator(predictionCol='prediction', labelCol='label',metricName='mse')
# evaluatorMae = RegressionEvaluator(predictionCol='prediction', labelCol='label',metricName='mae')



#clean data
df = df.withColumn("CountryMatch", regexp_replace(col("Country"), " ",""))



#create list of models to built
from pyspark.sql.functions import col
countryList = (df
               .where(col("Year") >= 2018)
               .select("Country","CountryMatch")
               .groupBy("Country","CountryMatch")
               .count()
               .rdd
               .map(lambda x: (x[0],x[1])).collect()
              )
countryList




#threading function for building models
def chunk(alist, wanted_parts=1):
    length = len(alist)
    return [ alist[i*length // wanted_parts: (i+1)*length // wanted_parts] 
             for i in range(wanted_parts) ]



#build models
def process(*countryList):
  for cntry in countryList:
    train=(df
           .where(col("Country") == cntry[0])
           .where(col("Year") > 2011)
           .where(col("Year") < 2019)
           .withColumnRenamed('qtyInterp_local','label')
           )
    print("starting:", cntry[1])
    gbtmodel = gbt_pipeline.fit(train)
    modelPath = "/FileStore/tables/ml_models/gbt_model" + cntry[1]
    gbtmodel.save(modelPath)
    print("ending:", cntry[1])

countryListArr = chunk(countryList, 10)
t1 = Thread(target=process, args=countryListArr[0])
t2 = Thread(target=process, args=countryListArr[1])
t3 = Thread(target=process, args=countryListArr[2])
t4 = Thread(target=process, args=countryListArr[3])
t5 = Thread(target=process, args=countryListArr[4])
t6 = Thread(target=process, args=countryListArr[5])
t7 = Thread(target=process, args=countryListArr[6])
t8 = Thread(target=process, args=countryListArr[7])
t9 = Thread(target=process, args=countryListArr[8])
t10 = Thread(target=process, args=countryListArr[9])
t1.start(); t2.start(); t3.start(); t4.start(); t5.start(); t6.start(); t7.start(); t8.start(); t9.start(); t10.start(); 
t1.join(); t2.join(); t3.join(); t4.join();t5.join(); t6.join();t7.join(); t8.join();t9.join(); t10.join();